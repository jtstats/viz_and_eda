---
title: "Exploratory Analysis Using group_by"
author: "Jingyi"
date: "10/7/2019"
output: html_document
---

Data sets can frequently be partitioned into meaningful groups based on the variables they contain. Making this grouping explicit allows the computation of numeric summaries within groups, which in turn facilitates quantitative comparisons.

```{r set up, include = FALSE}
# knitr will run the chunk but not include the chunk in the final document

# ensure reproductivity
set.seed(1)

# load library
library(tidyverse)
library(viridis)

knitr::opts_chunk$set(
  # display the code in the code truck above its results in the final document
  echo = TRUE,
  # do not display any warning messages generated by the code
  warning = FALSE,
  # set the figure to be 8 x 6, and the proportion it takes to be 90%
  fig.width = 8,
  fig.height = 6, 
  out.width = "90%"
)

# setting a global options for continuous data color family and a different format to set discrete data to have a color family
options(
  ggplot2.countinuous.colour = "viridis",
  ggplot2.countinuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# have a minimal theme and legends at the bottom
theme_set(theme_minimal() + theme(legend.position = "bottom"))

```

## **Example**

We’ll continue in the same Git repo / R project that we used for visualization, and use essentially the same weather_df dataset – the only exception is the addition of month variable, created using lubridate::floor_date().

```{r}

weather_df = 
  # load dataset for the specific three locations
  rnoaa::meteo_pull_monitors(c("USW00094728", "USC00519397", "USS0023B17S"),
                      # the variables are PRCP, TMIN and TMAX
                      var = c("PRCP", "TMIN", "TMAX"), 
                      # the date min and max
                      date_min = "2017-01-01",
                      date_max = "2017-12-31") %>%
  mutate(
    # rename the locations to these names
    name = recode(id, USW00094728 = "CentralPark_NY", 
                      USC00519397 = "Waikiki_HA",
                      USS0023B17S = "Waterhole_WA"),
    # take all tmin and tmax values and /10 to be their new values
    tmin = tmin / 10,
    tmax = tmax / 10,
    # add a month variable which only bring the date to their corresponding month but with a format of year-month-1 (round down)
    month = lubridate::floor_date(date, unit = "month")) %>%
  # reorder the arrangement to be name, id, date, month and then everything else
  select(name, id, date, month, everything())

```

## **group_by**

Datasets are often comprised of groups defined by one or more (categorical) variable; group_by() makes these groupings explicit so that they can be included in subsequent operations. For example, we might group weather_df by name and month:

```{r}

weather_df %>%
  # group the data by their name and month
  group_by(name, month)

```

Several important functions respect grouping structures. You will frequently use summarize to create one-number summaries within each group, or use mutate to define variables within groups. The rest of this example shows these functions in action.

Because these (and other) functions will use grouping information if it exists, it is sometimes necessary to remove groups using ungroup().

## **Counting things**

As an intro to summarize, let’s count the number of observations in each month in the complete weather_df dataset.

```{r}

weather_df %>%
  # group by month
  group_by(month) %>%
  # count the name of observations in each month
  summarize(n = n())

```

The result is a dataframe that includes the grouping variable and the desired summary.

In this case, you could use count() in place of group_by() and summarize() if you remember that this function exists. I’ll also make use of the name argument in count, which defaults to "n".

```{r}

weather_df %>%
  # another way by count(variable_name, and count the number of observation in that variable and display a new dataframe with name n_days and values will be their counts)
  count(month, name = "n_days")

```

count() is a useful tidyverse alternative to Base R’s table function. Both functions produce summaries of how often values appear, but table’s output is of class table and is hard to do any additional work with, while count produces a dataframe you can use or manipulate directly. For an example, run the code below and try to do something useful with the result…

```{r}

# this does not return a dataframe
weather_df %>%
  # bring the month out from weather_df dataframe
  pull(month) %>% 
  # use table to find their counts for each month
  table

```

You can use summarize() to compute multiple summaries within each group. As an example, we count the number of observations in each month and the number of distinct values of date in each month.

```{r}

weather_df %>%
  # group by month
  group_by(month) %>%
  # use summarize to create a fataframe with variable month, a new variable n_obs telling the count of each month, and another new variable n_days to show how many distinct days appeared in the data
  summarize(
    n_obs = n(),
    n_days = n_distinct(date))

```

## **(2 x 2 tables)**

You might find yourself, someday, wanting to tabulate the frequency of a binary outcome across levels of a binary predictor. In a contrived example, let’s say you want to look at the number of cold and not-cold days in Central Park and Waterhole. We can do this with some extra data manipulation steps and group_by + summarize:

```{r}

# in the weather_df
weather_df %>% 
  mutate(
    # add a new variable called cold
    cold = case_when(
      # put the value as cold when tmax is less than 5
      tmax < 5 ~ "cold",
      # put the value as not_cold when tmax is greater or equal to 5
      tmax >=5 ~ "not_cold",
      ############
      # QUESTION #
      ############
      TRUE     ~ ""
  )) %>% 
  filter(name != "Waikiki_HA") %>% 
  group_by(name, cold) %>% 
  summarize(count = n())

```

This is a “tidy” table, and it’s also a data frame. You could re-organize into a more standard (non-tidy) 2x2 table using pivot_wider, or you could use janitor::tabyl:

```{r}

weather_df %>% 
  # add a new variable called cold
  mutate(cold = case_when(
    # put the value as cold when tmax is less than 5
    tmax < 5 ~ "cold",
    # put the value as not_cold when tmax is greater or equal to 5
    tmax >=5 ~ "not_cold",
    TRUE     ~ ""
  )) %>% 
  # filter out waikiki_HA
  filter(name != "Waikiki_HA") %>% 
  # make it a 2x2 table (you can also use pivot_wider)
  janitor::tabyl(name, cold)

```

This isn’t tidy, but it is still a data frame – and that’s noticeably better than usual output from R’s built-in table function. janitor has a lot of little functions like this that turn out to be useful, so when you have some time you might read through all the things you can do. I don’t really love that this is called tabyl, but you can’t always get what you want in life.

(Since we’re on the subject, I think 2x2 tables are kind of silly. When are you ever going to actually analyze data in that format?? In grad school I thought I’d be computing odds ratios by hand everyday (OR=AD/BC, right?!), but really I do that as often as I write in cursive – which is never. Just do a logistic regression adjusting for confounders – because there are always confounders. And is a 2x2 table really that much better than the “tidy” version? There are 4 numbers.)

## **General summaries**

Standard statistical summaries are regularly computed in summarize() using functions like mean(), median(), var(), sd(), mad(), IQR(), min(), and max(). To use these, you indicate the variable to which they apply and include any additional arguments as necessary.

```{r}

weather_df %>%
  # group by month
  group_by(month) %>%
  # use summarize to find common statistics
  summarize(
    # mean of tmax
    mean_tmax = mean(tmax),
    # mean of prec and remove any NA values
    mean_prec = mean(prcp, na.rm = TRUE),
    # median of tmax
    median_tmax = median(tmax),
    # sd of tmax
    sd_tmax = sd(tmax))
# this return a dataframe with varible month, mean_tmax, mean_prec, median_tmax and sd_tmax

```

You can group by more than one variable.

```{r}

weather_df %>%
  # group by name and month
  group_by(name, month) %>%
  # use summarize to find some common stats
  summarize(
    # mean of tmax
    mean_tmax = mean(tmax),
    # median of tmax
    median_tmax = median(tmax))

```

The fact that summarize() produces a dataframe is important (and consistent with other functions in the tidyverse). You can incorporate grouping and summarizing within broader analysis pipelines. For example, we can take create a plot based on the monthly summary:

```{r}

weather_df %>%
  # group by name and month
  group_by(name, month) %>%
  # use summarize to find the mean of tmax
  summarize(mean_tmax = mean(tmax)) %>%
  # create a scatterplot with x = month and y = mean of tmax and assign color according to its name
  # line the dots
  # put the legend at the bottom
  ggplot(aes(x = month, y = mean_tmax, color = name)) + 
    geom_point() + geom_line() + 
    theme(legend.position = "bottom")

```

The results of group_by() and summarize() are generally tidy, but presenting reader-friendly results for this kind of exploratory analysis often benefits from some un-tidying. For example, the table below shows month-by-month average max temperatures in a more human-readable format.

```{r}

weather_df %>%
  # group by name and month
  group_by(name, month) %>%
  # use summarize to find mean of tmax
  summarize(mean_tmax = mean(tmax)) %>% 
  # put data into wide format for human readability
  pivot_wider(
    # put the names of each location (aka values of varible name) as new variables
    names_from = name,
    # put the values of mean_tmax to according cell
    values_from = mean_tmax) %>% 
  # keep only one decimal place
  knitr::kable(digits = 1)

```

## **Grouped mutate**

Summarizing collapses groups into single data points. In contrast, using mutate() in conjuntion with group_by() will retain all original data points and add new variables computed within groups.

Suppose you want to compare the daily max temperature to the annual average max temperature for each station separately, and to plot the result. You could do so using:

```{r}

weather_df %>%
  # group by name
  group_by(name) %>%
  # add a new variable called centered_tmax and put the values as the difference between tmax -mean(tmax)
  mutate(centered_tmax = tmax - mean(tmax)) %>% 
  # create a scatterplot with x-axis as date and y-axis as centered_tmax, assign colors according to name variable
  ggplot(aes(x = date, y = centered_tmax, color = name)) +
    geom_point() 

```

## **Window functions**

The previous example used mean() to compute the mean within each group, which was then subtracted from the observed max tempurature. mean() takes n inputs and produces a single output.

Window functions, in contrast, take n inputs and return n outputs, and the outputs depend on all the inputs. There are several categories of window functions; you’re most likely to need ranking functions and offsets, which we illustrate below.

First, we can find the max temperature ranking within month.

```{r}

weather_df %>%
  # group by name and month
  group_by(name, month) %>%
  # add a new variblae called temp_ranking and the values are the ranks from minimum to maximum in each group
  # with same value, comes the same rank, and will skip numbers(if there is 2 rank 22's, the next rank will be rank 24)
  mutate(temp_ranking = min_rank(tmax))

```

This sort of ranking is useful when filtering data based on rank. We could, for example, keep only the day with the lowest max temperature within each month:

```{r}

weather_df %>%
  # group by name and month
  group_by(name, month) %>%
  # keep only the lowest 2 temp days data in each group
  filter(min_rank(tmax) < 2)

```

We could also keep the three days with the highest max temperature:

```{r}

weather_df %>%
  # group by name and month
  group_by(name, month) %>%
  # keep only three top temp day's data
  # desc() - transform a vector into a format that will be sorted in descending order
  filter(min_rank(desc(tmax)) < 4)

```

In both of these, we’ve skipped a mutate() statement that would create a ranking variable, and gone straight to filtering based on the result.

Offsets, especially lags, are **used to compare an observation to it’s previous value**. This is useful, for example, to find the day-by-day change in max temperature within each station over the year:

```{r}

weather_df %>%
  # group by name
  group_by(name) %>%
  # add a variable called temp_change, and the values will be the difference of tmax of that date and the last value in the group
  mutate(temp_change = tmax - lag(tmax))

```

This kind of variable might be used to quantify the day-by-day variability in max temperature, or to identify the largest one-day increase:

```{r}

weather_df %>%
  # group by name
  group_by(name) %>%
  # add a variable called temp_change, and the values will be the difference of tmax of that date and the last value in the group
  mutate(temp_change = tmax - lag(tmax)) %>%
  # use summarize to find:
            # sd of temp_change and remove NA values for temp_change
  summarize(temp_change_sd = sd(temp_change, na.rm = TRUE),
            # max of temp_change and remove NA values for temp_change
            temp_change_max = max(temp_change, na.rm = TRUE))

# This returns a table with 3 (names) x 2 (temp_change_sd, temp_change_max)

```

## **Limitations**

summarize() can only be used with functions that return a single-number summary. This creates a ceiling, even if it is very high. Later we’ll see how to aggregate data in a more general way, and how to perform complex operations on the resulting sub-datasets.

## **Revisiting examples**

We’ve seen the PULSE and FAS datasets on several occasions, and we’ll briefly revisit them here.

### **Learning Assessment:** 

In the PULSE data, the primary outcome is BDI score; it’s observed over follow-up visits, and we might ask if the typical BDI score values are roughly similar at each. Try to write a code chunk that imports, cleans, and summarizes the PULSE data to examine the mean and median at each visit. Export the results of this in a reader-friendly format.

```{r}

# The code chunk below imports and tidies the PUSLE data, produces the desired information, and exports it using knitr::kable.

# create a pulse_data
pulse_data = 
  # read data
  haven::read_sas("./data/public_pulse_data.sas7bdat") %>%
  # clean names
  janitor::clean_names() %>%
  # switch the data to its long format
  pivot_longer(
    # choose all variables from bdi_score_bl to bdi_score_12m
    bdi_score_bl:bdi_score_12m,
    # add a new variable called visit and the values are bdi_score_bl, bdi_score_12m and so on
    names_to = "visit", 
    # get rid of the prefix "bdi_score_"
    names_prefix = "bdi_score_",
    # add a new variable called bdi and fill the values of the original data to their category
    values_to = "bdi") %>%
  # put id and visit and then everything else as the new order of the data frame
  select(id, visit, everything()) %>%
  mutate(
    # replace value "bl" in visit variable to "00m"
    visit = replace(visit, visit == "bl", "00m"),
    # make the visit as factor variable, which levels are 00m, 01m, 06m and 12m
    visit = factor(visit, levels = str_c(c("00", "01", "06", "12"), "m"))) %>%
  # arrange the data according to their id and visit values ascendingly
  arrange(id, visit)

# In the pulse dataset
pulse_data %>% 
  # group by visit
  group_by(visit) %>% 
  # use summarize to find common stats
  summarize(
    # add new mean_bdi variable for mean of bdi and remove missing values
    mean_bdi = mean(bdi, na.rm = TRUE),
    # add new median_bdi variable for median of bdi and remove missing values
    median_bdi = median(bdi, na.rm = TRUE)) %>% 
  # export the output as a table, and keep 3 decimal places
  knitr::kable(digits = 3)

# This quick summary suggests a relatively large drop in the typical BDI score from baseline to 1 month, with small or no changes thereafter.

```

## **Learning Assessment:** 

In the FAS data, there are several outcomes of interest; for now, focus on post-natal day on which a pup is able to pivot. Two predictors of interest are the dose level and the day of treatment. Produce a reader-friendly table that quantifies the possible associations between dose, day of treatment, and the ability to pivot.

```{r}

# The code chunk below imports the litters and pups data, joins them, produces the desired information, un-tidies the result, and exports a table using knitr::kable.

pup_data = 
  # read pups data
  read_csv("./data/FAS_pups.csv", col_types = "ciiiii") %>%
  # clean the names
  janitor::clean_names() %>%
  # for sex variable let "1" be male and "2" be female and change it in the dataframe
  mutate(sex = recode(sex, `1` = "male", `2` = "female")) 

litter_data = 
  # read litters data
  read_csv("./data/FAS_litters.csv", col_types = "ccddiiii") %>%
  # clean the names
  janitor::clean_names() %>%
  # select all other than pups_survive 
  select(-pups_survive) %>%
  # separate group variable values to two variables called dose and day_of_tx, separate all values from the thrid place in the value
  separate(group, into = c("dose", "day_of_tx"), sep = 3) %>%
  # add a variable called wt_gain and the values are the difference between gd18_weight and gd0_weight
  mutate(wt_gain = gd18_weight - gd0_weight,
         # change day_of_tx values to numeric data type
         day_of_tx = as.numeric(day_of_tx))

# left join pup_data to litter_data, and the key is litter_number
fas_data = left_join(pup_data, litter_data, by = "litter_number") 

fas_data %>% 
  # group by dose and day of tx
  group_by(dose, day_of_tx) %>% 
  # Use summarize to find mean for pivot for each group and remove any missing values
  summarize(mean_pivot = mean(pd_pivot, na.rm = TRUE)) %>%
  # filter out all values that are not NA for dose
  filter(!is.na(dose)) %>% 
  # same as pivot_wider
  # df %>% spread(key, value) is equivalent to df %>% pivot_wider(names_from = key, values_from = value)
  # put values of dose as new variables, and fit the mean_pivot according to the category in each cell
  spread(key = dose, value = mean_pivot) %>% 
  # knit a table in R markdown and keep to 3 decimal places
  knitr::kable(digits = 3)

# These results may suggest that pups in the control group are able to pivot earlier than pups in the low-dose group, but it is unclear if there are differences between the control and moderate-dose groups or if day of treatment is an important predictor.

```

_**Note:**_ In both of these examples, the data are structure such that repeated observations are made on the same study units. In the PULSE data, repeated observations are made on subjects over time; in the FAS data, pups are “repeated observations” within litters. The analyses here, and plots made previously, are exploratory – any more substantial claims would require appropriate statistical analysis for non-independent samples
